<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning">
  <meta property="og:title" content="ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning">
  <meta name="twitter:description" content="ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ARRANGE: Adaptive Referring Relation Alignment Network for Open-domain Multi-task Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
<!--               <sup>*</sup> -->
<!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zhiwei Chen<sup>1</sup></a>,</span> -->
              <span class="author-block">
                <span>Zhiwei Chen<sup>1</sup></span>,
              </span>
              <span class="author-block">
                <span>Zixu Li<sup>1</sup></span>,
              </span>
              <span class="author-block">
                <span>Zhiheng Fu<sup>1</sup></span>,
              </span>
              <span class="author-block">
                <span>Yinwei Wei<sup>2</sup></span>,
              </span>
              <span class="author-block">
                <span>Yupeng Hu<sup>1</sup></span>,
              </span>
              <span class="author-block">
                <span>Weili Guan<sup>3</sup></span>,
              </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Shandong University,<sup>2</sup>Monash University,<sup>3</sup>Harbin Institute of Technology (Shenzhen)<br>Date: 2024-11</span>
<!--                     <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                  <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/1I-7xLv07a5h0Xwv8ba2eiNQt3QeRzDwQ?usp=sharing" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google-drive"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual-linguistic pretraining models have been widely applied in numerous visual and linguistic (V+L) downstream tasks due to their outstanding cross-modal comprehension capabilities, and significant progress has also been made in multi-task learning models tailored for different V+L tasks. However, due to the discrepancy and comprehension difficulty of visual and textual contents in diverse domains, existing studies tend to focus on V+L multi-task learning for specific domains and neglect that for open domains. Nevertheless, this is non-trivial due to the following two challenges: 1) referring granularity discrepancy, and 2) alignment specificity. To address the above challenges, we propose a novel V+L multi-task learning approach (ARRANGE) that can accommodate both specific-domain and open-domain scenarios. Compared to existing methods, ARRANGE employs a unified model to handle multiple heterogeneous V+L tasks and thus exhibits high parameter efficiency and domain generalization. ARRANGE generates referring tokens through unified referring embedding and learns the alignment specificity of individual tasks to adaptively align visual and textual referring. Extensive experiments on six benchmark datasets (covering both specific and open domains) for three V+L tasks show that our proposed ARRANGE significantly outperforms previous specific-domain-oriented V+L multi-task learning methods and surpasses traditional single-task models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-fifths">
        <h2 class="title is-3">Tasks</h2>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/Intro.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Illustrations of the V+L tasks studied in this work.
          </h2>
          </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  has-text-centered">
        <h2 class="title is-3">Framework</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/Framework.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            The proposed ARRANGE consists of three primary modules: (a) Uniform Feature Extraction, (b) Referring Token Embedding, and (c) Task-free Alignment Learning, followed by Downstream Task Optimization.
          </h2>
          </div>

  <!-- </div> -->
</div>
</div>
</section>
<!-- End image carousel -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container  has-text-centered">
      <h2 class="title is-3">Experiment</h2>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/ITM.png" alt="MY ALT TEXT"/>
        </div>
        <br>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/TGIR.png" alt="MY ALT TEXT"/>
        </div>
        <br>
        <div class="item">
          <!-- Your image here -->
          <div class="columns is-centered has-text-centered">
            <div class="column is-two-fifths">
              <img src="static/images/SPML.png" alt="MY ALT TEXT"/>
            </div>

          </div>
        </div>
        <br>
        <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <img src="static/images/Attention.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Attention Visualization on (a) ITM, (b) TGIR, and (c) SPML tasks.
          </h2>
        </div>
      </div>
  <!-- </div> -->
</div>
</div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
